{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import os\n",
    "import os.path as path\n",
    "from os.path import dirname as up\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from pathlib import Path   \n",
    "import pmdarima as pm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from pmdarima import auto_arima\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute Paths\n",
    "master_folder = up(up(os.path.abspath(os.getcwd()))) + \"\\\\\"\n",
    "production_pipeline = \"production_pipeline\\\\\"\n",
    "data_folder = \"data\\\\\"\n",
    "code_folder = \"code\\\\\"\n",
    "raw_data_folder = \"raw_data\\\\\"\n",
    "processed_input_data = \"processed_input_data\\\\\"\n",
    "temp_folder = \"temp_input_data\\\\\"\n",
    "store_prd = \"StoreProduct.csv\"\n",
    "raw_data = \"ExpressTop50.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeseries_evaluation_metrics_func(y_true, y_pred, store_id, prd_id):\n",
    "    global perf_df\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        return mape\n",
    "    \n",
    "    \n",
    "    diffs = y_true - y_pred\n",
    "    sum_diff = np.sum(diffs)\n",
    "    sum_act = np.sum(y_true)\n",
    "    wmape = sum_diff/sum_act\n",
    "    mape = metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    pdf = {\"Store\" : store_id,\"Product\": prd_id,\"RMSE\": rmse,\"MAPE\": mape}\n",
    "    perf_df = perf_df.append(pd.DataFrame([pdf]))\n",
    "\n",
    "    # print('Evaluation metric results:-')\n",
    "    # print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')\n",
    "    # print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')\n",
    "    # print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')\n",
    "    # print(f'MAPE is : {metrics.mean_absolute_percentage_error(y_true, y_pred)}')\n",
    "    # print(f'WMAPE is : {wmape}')\n",
    "    # print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holt_winters(df,store_id,p_id):\n",
    "    # Set the frequency of the date time index as weekly start as indicated by the data\n",
    "    # Set the value of Alpha and define m (Time Period)\n",
    "    m = 7\n",
    "    alpha = 1/(2*m)\n",
    "    # Creating dataframe with only date and sales columns\n",
    "    df_pred = pd.DataFrame(df, columns=['quantity','day_dt','wk_end_dt'])\n",
    "    # Test and train split (80-20)\n",
    "\n",
    "    # Dataset consists of day_dt, wk_end_dt and mkt_bskt_ut_qt columns\n",
    "    # Converting date columns to datetime format\n",
    "    df_pred['day_dt'] = pd.to_datetime(df_pred['day_dt']) - pd.to_timedelta(0, unit='d')\n",
    "    df_pred['wk_end_dt'] = pd.to_datetime(df_pred['wk_end_dt']) - pd.to_timedelta(0, unit='d')\n",
    "    # Aggregate to weekly level following a Saturday to Friday schedule\n",
    "    df_pred = df_pred.groupby([pd.Grouper(key='wk_end_dt', freq='W-SAT')])['quantity'].sum().reset_index()\n",
    "\n",
    "    split = int(len(df_pred)*0.8)\n",
    "    train = df_pred[:split]\n",
    "    train = train.set_index('wk_end_dt')\n",
    "    test = df_pred[split:]\n",
    "    test = test.set_index('wk_end_dt')\n",
    "\n",
    "    # display(test_weekly.head())\n",
    "    # Holt-Winters' Model\n",
    "    fitted_model = ExponentialSmoothing(train['quantity'],trend='add',seasonal='add',seasonal_periods=12).fit()\n",
    "    test_predictions = fitted_model.forecast(len(test))\n",
    "    # train['quantity'].plot(legend=True,label='TRAIN')\n",
    "    # test['quantity'].plot(legend=True,label='TEST',figsize=(6,4))\n",
    "    # test_predictions.plot(legend=True,label='PREDICTION')\n",
    "    # plt.title('Train, Test and Predicted Test using Holt Winters')\n",
    "    # Error Metrics\n",
    "    # print(f'Mean Absolute Error = {mean_absolute_error(test,test_predictions)}')\n",
    "    # print(f'Mean Absolute Percentage Error = {mean_absolute_percentage_error(test,test_predictions)}')\n",
    "    # print(f'Mean Squared Error = {mean_squared_error(test,test_predictions)}')\n",
    "    timeseries_evaluation_metrics_func(test.to_numpy(),test_predictions.to_numpy(),store_id,p_id)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(df,store_id,p_id):\n",
    "    df['day_dt'] = pd.to_datetime(df['day_dt'])\n",
    "    df = df.drop(['store_id','ut_same_sto_flg','prd_id','wk_end_dt','mjr_mds_are_id','mjr_mds_are_id','mjr_mds_are_id','mjr_mds_are_id','mjr_p_cls_id'],axis = 1)\n",
    "    df2_date = df['day_dt']\n",
    "\n",
    "    split = int(len(df)* 0.8)    \n",
    "    df2_train = df.iloc[0:split,1:]\n",
    "    df2_test = df.iloc[split:,1:]\n",
    "    sc = StandardScaler()\n",
    "    df2_train_scaled = sc.fit_transform(df2_train)\n",
    "    print('Train Scaled Shape: ',df2_train_scaled.shape)\n",
    "    sc2 = StandardScaler()\n",
    "    df2_train_scaled_y = sc2.fit_transform(df2_train[['quantity']])\n",
    "    print('Test Scaled Shape: ',df2_train_scaled_y.shape)\n",
    "    hops=14    \n",
    "    no_records = split    \n",
    "    no_cols = df2_train_scaled.shape[1]\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(14,split):\n",
    "        X_train.append(df2_train_scaled[i-14:i])\n",
    "        y_train.append(df2_train_scaled_y[i][0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_train_shape = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,return_sequences=True, input_shape=(14,df2_train_scaled.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss ='mean_squared_error')\n",
    "    model.fit(X_train_shape,y_train,epochs = 90, batch_size = 100)\n",
    "    df1_train_last14 = df2_train.iloc[-14:]\n",
    "    df1_test_full = df2_test    \n",
    "    full_df = pd.concat((df1_train_last14, df1_test_full),axis=0)\n",
    "    full_df = sc.transform(full_df)\n",
    "    no_records = full_df.shape[0]\n",
    "    no_cols = full_df.shape[1]\n",
    "    X_train_shape_pred = []\n",
    "    for i in range(hops,no_records):\n",
    "        X_train_shape_pred.append(full_df[i-14:i])\n",
    "    X_train_shape_pred = np.array(X_train_shape_pred)\n",
    "    ytest = model.predict(X_train_shape_pred)\n",
    "    y_final_pred = sc2.inverse_transform(ytest)\n",
    "    final_open_pred = pd.DataFrame(y_final_pred)\n",
    "    final_open_pred.columns = ['final_open_pred']\n",
    "    final_open_pred.index += split\n",
    "    fully_final = pd.concat((final_open_pred,df2_test),axis = 1)\n",
    "    fully_final = fully_final[['quantity','final_open_pred', 'atl_max_temp_val', 'atl_mean_temp_val', \n",
    "       'atl_min_temp_val', 'atl_p_sold_am', 'atl_t_ppt_qt', 'atl_t_snow_qt',\n",
    "       'blackfri', 'cldr_day_of_wk_id', 'columbus', 'dad', 'dadsat',\n",
    "       'day_aft_ppt_temp_val', 'day_aft_snow_temp_val', 'day_bef_ppt_temp_val',\n",
    "       'day_bef_snow_temp_val', 'dayb4valentine', 'dectwosix', 'dectwothree',\n",
    "       'easter', 'eastersat', 'easterwk', 'fridayb4turkey', 'halloween',\n",
    "       'halloweeneve', 'halloweenfri', 'halloweenwkend',\n",
    "       'HomeLocationCapacityQuantity', 'labor', 'laborfri', 'laborsat',\n",
    "       'laborsun', 'laborwkend', 'mcc_blk_out', 'memorial', 'memwkend',\n",
    "        'mlk', 'mom', 'momsat', 'ny', 'nyeve',\n",
    "       'p_promo_pr_am', 'p_sell_dur', 'prehalo_fri', 'prehalo_sat',\n",
    "       'prehalo_sun', 'presday', 'redsat', 'satpny', 'saturdayb4turkey',\n",
    "       'sunpny', 'sunprexmas', 'superbowl', 'superbowlsat', 'turkey',\n",
    "       'valentine', 'valentinewknd', 'veterans', 'xmaseve']]\n",
    "    fully_final['final_open_pred'] = fully_final['final_open_pred'].mask(fully_final['final_open_pred'] < 0, 0)\n",
    "    plt.plot(df['quantity'], label = 'actual', color = 'red')\n",
    "    plt.plot(fully_final['final_open_pred'], label = 'predicted', color = 'blue')\n",
    "    plt.legend()\n",
    "    Y_actual = fully_final['quantity'] \n",
    "    Y_Predicted = fully_final['final_open_pred']   \n",
    "    return timeseries_evaluation_metrics_func(Y_actual.to_numpy(),Y_Predicted.to_numpy(),store_id,p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is file filtered by store and product, set with weekly index but containing all columns.\n",
    "def arima_week(df,store_id,p_id):    \n",
    "    #columns of interest\n",
    "    cols = ['quantity','atl_p_sold_am', 'atl_mean_temp_val','wk_end_dt'] \n",
    "    df_sel = df[cols]\n",
    "    \n",
    "    #weekly aggregation\n",
    "    week = df_sel.groupby('wk_end_dt').aggregate({'quantity':'sum', 'atl_p_sold_am': 'mean', 'atl_mean_temp_val': 'mean'})\n",
    "    \n",
    "    #post covid data filter\n",
    "    el_df = week[week.index.get_level_values(0) > '2020-07-12']\n",
    "    \n",
    "    #data partition\n",
    "    split = int(len(el_df)*0.8)\n",
    "    train = el_df[:split]\n",
    "    test = el_df[split:]\n",
    "    # train, test = model_selection.train_test_split(el_df, train_size=0.8)\n",
    "\n",
    "    # Fit a simple auto_arima model\n",
    "    arima = pm.auto_arima(train['quantity'], error_action='ignore', trace=False,\n",
    "                      suppress_warnings=True, maxiter=5,\n",
    "                      seasonal=True, m=12)\n",
    "    \n",
    "    preds, conf_int = arima.predict(n_periods=test.shape[0], return_conf_int=True)\n",
    "    # Plot actual test vs. forecasts:\n",
    "    # plt.plot(test.index, test['quantity'], marker='o', label = 'Actual Test Samples')\n",
    "    # plt.plot(test.index, arima.predict(n_periods=test.shape[0]), color='green', label= 'Forecast Test Samples')\n",
    "    # plt.title('Actual test samples vs. forecasts')\n",
    "    # plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    \n",
    "    #return RMSE, MAPE, WMAPE\n",
    "    return timeseries_evaluation_metrics_func(test['quantity'].to_numpy(),preds.to_numpy(),store_id,p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo = ['p_sell_dur',\n",
    " 'p_bse_pr_am',\n",
    " 'p_promo_pr_am',\n",
    " 'atl_p_sold_am',\n",
    " 'atl_mean_temp_val',\n",
    " 'atl_t_ppt_qt',\n",
    " 'atl_t_snow_qt',\n",
    " 'day_bef_ppt_temp_val',\n",
    " 'day_bef_snow_temp_val',\n",
    " 'day_aft_ppt_temp_val',\n",
    " 'day_aft_snow_temp_val',\n",
    " 'sunprexmas',\n",
    " 'redsat',\n",
    " 'blackfri',\n",
    " 'turkey',\n",
    " 'saturdayb4turkey',\n",
    " 'fridayb4turkey',\n",
    " 'veterans',\n",
    " 'halloweenfri',\n",
    " 'halloween',\n",
    " 'halloweeneve',\n",
    " 'prehalo_sun',\n",
    " 'prehalo_fri',\n",
    " 'columbus',\n",
    " 'laborfri',\n",
    " 'laborsun',\n",
    " 'laborsat',\n",
    " 'laborwkend',\n",
    " 'labor',\n",
    " 'dad',\n",
    " 'dadsat',\n",
    " 'memwkend',\n",
    " 'mom',\n",
    " 'momsat',\n",
    " 'easterwk',\n",
    " 'easter',\n",
    " 'eastersat',\n",
    " 'presday',\n",
    " 'valentinewknd',\n",
    " 'valentine',\n",
    " 'dayb4valentine',\n",
    " 'superbowlsat',\n",
    " 'superbowl',\n",
    " 'mlk',\n",
    " 'ny',\n",
    " 'julyfour',\n",
    " 'halloweenwkend',\n",
    " 'dectwothree',\n",
    " 'xmaseve',\n",
    " 'satpny',\n",
    " 'sunpny',\n",
    " 'memorial',\n",
    " 'prehalo_sat',\n",
    " 'turkeywed',\n",
    " 'dectwosix',\n",
    " 'mcc_blk_out',\n",
    " 'nyeve',\n",
    " 'cldr_day_of_wk_id']\n",
    "#  'dow1',\n",
    "#  'dow2',\n",
    "#  'dow3',\n",
    "#  'dow4',\n",
    "#  'dow5',\n",
    "#  'dow6',\n",
    "#  'dow7',\n",
    "#  'wk1',\n",
    "#  'wk2',\n",
    "#  'wk3',\n",
    "#  'wk4',\n",
    "#  'wk5',\n",
    "#  'wk6',\n",
    "#  'wk7',\n",
    "#  'wk8',\n",
    "#  'wk9',\n",
    "#  'wk10',\n",
    "#  'wk11',\n",
    "#  'wk12',\n",
    "#  'wk13',\n",
    "#  'wk14',\n",
    "#  'wk15',\n",
    "#  'wk16',\n",
    "#  'wk17',\n",
    "#  'wk18',\n",
    "#  'wk19',\n",
    "#  'wk20',\n",
    "#  'wk21',\n",
    "#  'wk22',\n",
    "#  'wk23',\n",
    "#  'wk24',\n",
    "#  'wk25',\n",
    "#  'wk26',\n",
    "#  'wk27',\n",
    "#  'wk28',\n",
    "#  'wk29',\n",
    "#  'wk30',\n",
    "#  'wk31',\n",
    "#  'wk32',\n",
    "#  'wk33',\n",
    "#  'wk34',\n",
    "#  'wk35',\n",
    "#  'wk36',\n",
    "#  'wk37',\n",
    "#  'wk38',\n",
    "#  'wk39',\n",
    "#  'wk40',\n",
    "#  'wk41',\n",
    "#  'wk42',\n",
    "#  'wk43',\n",
    "#  'wk44',\n",
    "#  'wk45',\n",
    "#  'wk46',\n",
    "#  'wk47',\n",
    "#  'wk48',\n",
    "#  'wk49',\n",
    "#  'wk50',\n",
    "#  'wk51',\n",
    "#  'wk52',\n",
    "#  'HomeLocationCapacityQuantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarimax(df,store_id,p_id):\n",
    "    data3 = df\n",
    "    data3 = data3.reset_index(level=0)\n",
    "    data3.set_index('day_dt',inplace = True)\n",
    "\n",
    "    train_no = int(0.8 * len(data3))\n",
    "    # Create Training and Test\n",
    "    train = data3[:train_no]\n",
    "    test = data3[train_no:]\n",
    "\n",
    "    exog_forecast =  exo\n",
    "\n",
    "    model = SARIMAX(train['quantity'],exog=train[exog_forecast],order=(0,0,0),seasonal_order=(1,0,[1],7),enforce_invertibility=False)\n",
    "    results = model.fit()\n",
    "    # results.summary()\n",
    "\n",
    "    start = len(train)\n",
    "    end = len(train) + len(test) - 1\n",
    "\n",
    "    predictions =  results.predict(start=start,end=len(df)-1,exog=test[exog_forecast]).rename('SARIMAX(0,0,0)(1,0,[1],7) Predictions')\n",
    "    # display(predictions)\n",
    "    test['prediction']= predictions.tolist()\n",
    "    # print(predictions)\n",
    "    # display(test)\n",
    "\n",
    "    # f = plt.figure()\n",
    "    # f.set_figwidth(20)\n",
    "    # f.set_figheight(7)\n",
    "    # plt.plot(test['prediction'], color='green',label='Predicted')\n",
    "    # plt.plot(test[\"quantity\"], color='yellow', label='Actual')\n",
    "    # plt.xticks(rotation=90)\n",
    "    # # spacing = 1\n",
    "    # # plt.subplots_adjust(bottom=spacing)\n",
    "    # # plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right', fontsize='x-small')\n",
    "    # plt.legend(loc='best')\n",
    "\n",
    "    # test.index += train_no    \n",
    "    # plt.plot(data3.index, data3['quantity'], marker='o', label = 'Actual Test Samples')\n",
    "    # plt.plot(test.index, test['prediction'], color='green', label= 'Forecast Test Samples')\n",
    "    \n",
    "   \n",
    "    # plt.title('Actual test samples vs. forecasts')\n",
    "    # plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    return timeseries_evaluation_metrics_func(test[\"quantity\"].to_numpy(),predictions.to_numpy(),store_id,p_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(df,store_id,p_id):\n",
    "        import xgboost as xgb\n",
    "        from xgboost import plot_importance, plot_tree\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "        # df = x\n",
    "        split = int(len(df)*0.8)\n",
    "        train = df[:split]\n",
    "        test = df[split:]\n",
    "        train.set_index('day_dt',inplace = True)\n",
    "        test.set_index('day_dt',inplace = True)\n",
    "\n",
    "        X_train = train[exo]\n",
    "        y_train = train[['quantity']]\n",
    "        X_test = test[exo]\n",
    "        y_test = test[['quantity']]\n",
    "\n",
    "        reg = xgb.XGBRegressor(n_estimators=1000, \n",
    "                        learning_rate=0.03,\n",
    "                        max_depth=12,\n",
    "                        subsample=0.9,\n",
    "                        colsample_bytree=0.7,\n",
    "                        random_state=1111,\n",
    "                        tree_method='gpu_hist')\n",
    "        reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                early_stopping_rounds=50,\n",
    "        verbose=False)\n",
    "\n",
    "        # plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "        # _ = plot_importance(reg, height=3)\n",
    "        y_pred = reg.predict(X_test)\n",
    "\n",
    "        # plt.plot(y_test.index, y_test, label = 'actual', color = 'red')\n",
    "        # plt.plot(y_pred.index, y_pred, label = 'predicted', color = 'blue')\n",
    "        # plt.legend()\n",
    "        # _ = data_all[['number','number_Prediction']].plot(figsize=(15, 5))\n",
    "        timeseries_evaluation_metrics_func(y_test.to_numpy(),y_pred,store_id,p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_data(start_date):\n",
    "    str_prd = pd.read_csv(master_folder+production_pipeline+data_folder+raw_data_folder+store_prd)\n",
    "    str_prd = str_prd[[\"p_id\",\"ut_id\"]]\n",
    "    str_prd = str_prd.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    for index, row in str_prd.iterrows():\n",
    "        data = pd.read_csv(master_folder+production_pipeline+data_folder+raw_data_folder + raw_data,chunksize=1000000)\n",
    "        l = []\n",
    "        for i in data:\n",
    "            # if index == 0:\n",
    "            #     i['day_dt'] = pd.to_datetime(i['day_dt'])\n",
    "            \n",
    "            i = i.loc[(i[\"p_id\"]==row['p_id']) & (i[\"ut_id\"]==row['ut_id'])]\n",
    "            l.append(i)\n",
    "        \n",
    "        data = pd.concat(l, ignore_index=True)\n",
    "        data = data.loc[data[\"day_dt\"]>start_date]\n",
    "        \n",
    "        # Cleaning and pre-processing data\n",
    "\n",
    "        data.rename(\n",
    "            {\n",
    "                'ut_id': 'store_id', \n",
    "                'p_id': 'prd_id',\n",
    "                'mkt_bskt_ut_qt':'quantity'\n",
    "            }, axis=1, inplace=True)\n",
    "        \n",
    "        data = data.drop_duplicates(subset = [\"day_dt\",\"store_id\",\"prd_id\"])\n",
    "\n",
    "\n",
    "        xgboost(data,row['ut_id'],row['p_id'])\n",
    "        sarimax(data,row['ut_id'],row['p_id'])\n",
    "        arima_week(data,row['ut_id'],row['p_id'])\n",
    "        holt_winters(data,row['ut_id'],row['p_id'])\n",
    "        lstm(data,row['ut_id'],row['p_id'])\n",
    "\n",
    "        print(\"Done processing: Store - \" + str(row['ut_id']) + \" Product - \" + str(row['p_id']))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_file_data(\"2020-07-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency -1D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency -1D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "c:\\Users\\lavan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:834: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\lavan\\AppData\\Local\\Temp\\ipykernel_9224\\405279341.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['prediction']= predictions.tolist()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Personal Lavanya\\Purdue\\MSBAIM\\IP\\production_pipeline\\code\\modelling_v2.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_all_file_data(\u001b[39m\"\u001b[39;49m\u001b[39m2020-07-12\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Personal Lavanya\\Purdue\\MSBAIM\\IP\\production_pipeline\\code\\modelling_v2.ipynb Cell 12\u001b[0m in \u001b[0;36mget_all_file_data\u001b[1;34m(start_date)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdrop_duplicates(subset \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mday_dt\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mstore_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mprd_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m xgboost(data,row[\u001b[39m'\u001b[39m\u001b[39mut_id\u001b[39m\u001b[39m'\u001b[39m],row[\u001b[39m'\u001b[39m\u001b[39mp_id\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m sarimax(data,row[\u001b[39m'\u001b[39;49m\u001b[39mut_id\u001b[39;49m\u001b[39m'\u001b[39;49m],row[\u001b[39m'\u001b[39;49m\u001b[39mp_id\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m arima_week(data,row[\u001b[39m'\u001b[39m\u001b[39mut_id\u001b[39m\u001b[39m'\u001b[39m],row[\u001b[39m'\u001b[39m\u001b[39mp_id\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m holt_winters(data,row[\u001b[39m'\u001b[39m\u001b[39mut_id\u001b[39m\u001b[39m'\u001b[39m],row[\u001b[39m'\u001b[39m\u001b[39mp_id\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32mc:\\Personal Lavanya\\Purdue\\MSBAIM\\IP\\production_pipeline\\code\\modelling_v2.ipynb Cell 12\u001b[0m in \u001b[0;36msarimax\u001b[1;34m(df, store_id, p_id)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m test[\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(predictions)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# display(test)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# plt.legend()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# plt.show()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mreturn\u001b[39;00m timeseries_evaluation_metrics_func(test[\u001b[39m\"\u001b[39;49m\u001b[39mquantity\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto_numpy(),predictions\u001b[39m.\u001b[39;49mto_numpy(),store_id,p_id)\n",
      "\u001b[1;32mc:\\Personal Lavanya\\Purdue\\MSBAIM\\IP\\production_pipeline\\code\\modelling_v2.ipynb Cell 12\u001b[0m in \u001b[0;36mtimeseries_evaluation_metrics_func\u001b[1;34m(y_true, y_pred, store_id, prd_id)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(metrics\u001b[39m.\u001b[39mmean_squared_error(y_true, y_pred))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pdf \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mStore\u001b[39m\u001b[39m\"\u001b[39m : store_id,\u001b[39m\"\u001b[39m\u001b[39mProduct\u001b[39m\u001b[39m\"\u001b[39m: prd_id,\u001b[39m\"\u001b[39m\u001b[39mRMSE\u001b[39m\u001b[39m\"\u001b[39m: rmse,\u001b[39m\"\u001b[39m\u001b[39mMAPE\u001b[39m\u001b[39m\"\u001b[39m: mape}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Personal%20Lavanya/Purdue/MSBAIM/IP/production_pipeline/code/modelling_v2.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m perf_df \u001b[39m=\u001b[39m perf_df\u001b[39m.\u001b[39;49mappend(pd\u001b[39m.\u001b[39mDataFrame([pdf]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'append'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
